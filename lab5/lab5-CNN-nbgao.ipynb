{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 实验5 PyTorch CNN 实战\n",
    "\n",
    "Author: 高鹏昺\n",
    "\n",
    "Email: nbgao@126.com"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn, optim\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision as tv\n",
    "from torchstat import stat\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.transforms import ToPILImage\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters\n",
    "batch_size = 64\n",
    "learning_rate = 1e-2\n",
    "num_epoches = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_tf = transforms.Compose(\n",
    "    [transforms.ToTensor(),\n",
    "    transforms.Normalize([0.5],[0.5])]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: .\n",
      "    Split: Train\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.MNIST(root='.', train=True, transform=data_tf, download=False)\n",
    "print(train_dataset)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ./\n",
      "    Split: Test\n"
     ]
    }
   ],
   "source": [
    "test_dataset = datasets.MNIST(root='./', train=False, transform=data_tf, download=False)\n",
    "print(test_dataset)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "classes = ['0', '1', '2', '3', '4', '5', '6', '7', '8', '9']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 28, 28])\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "(data, label) = train_dataset[100]\n",
    "print(data.shape)\n",
    "print(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "show = ToPILImage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAIwAAACMCAAAAACLqx7iAAAB6ElEQVR4nO3av+tNcRzH8cdXyiCSDH5Npu8gk3/BzmBhMSqGb4nBZLHKoCS7yX/AgkUGEiaDLAoljEKW0/Ed7ufr3M896T28nsPtnj6f8+nZq1e387mfQwghhBBCCCGEEML/YG2eFfbAYTg7DlyEnfAdrsCdLZfatrLMjESmRSmZ7Z337YaTcALOLJjyDT4wFPjBvxctlUxkWpSS6S3wZbi6YOArvIUNeLrMoqWSiUyLUjIdBb7L5ieFHwx9fgOf4XWfTKlkItOilEzHM/ALODZefoSDs8iUSiYyLUrJdPwCP2dzgW/P5lIrmci0KCXTUeCHcA5+MWlzNpVSyUSmRSmZ3k0chgIvtU3bmlLJRKZFZFpEpkVkWkSmRWRalJLp+BdiH7yCvbAO72aRKZVMZFqUkuk9Sn7PcHb8Cb6MA/fgFsNJ3FKUSiYyLUrJ9Bb4PpxqjT6Ca/B4mUVLJROZFqVkegu8BpcYTt2Ow2k4Ok65OU6ZSqlkItOilMzqb6P9ZT88gSPwkqHZvycuUCqZyLQoJTNngcF5uAE7xo+fE+8tlUxkWpSSmb3AGF7rWScFnonItFjpJG4RB2BX372lkolMi1Iysxf4Ahxi2N1NffxFsWQi06KUzOwFfjZ+u04KPBORCSGEUI0/T8kyNd/JUh0AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<PIL.Image.Image image mode=L size=140x140 at 0x7F7E24212D30>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "img = ((data+1)/2)\n",
    "show(img).resize((140,140))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 5 6 5 0 0 8 7 7 1 9 6 9 0 9 4 3 9 4 1 0 5 1 6 5 4 9 2 3 0 3 5 5 0 7 6 5 6 3 0 1 3 2 4 8 3 1 3 9 3 1 2 8 9 9 0 7 6 4 1 1 1 6 6\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<PIL.Image.Image image mode=RGB size=400x400 at 0x7F7DAB14C160>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_iter = iter(train_loader)\n",
    "images, labels = data_iter.next()\n",
    "print(' '.join('%s'%classes[labels[j]] for j in range(64)))\n",
    "show(tv.utils.make_grid((images+1)/2)).resize((400,400))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU num: 1\n",
      "GPU id: 0\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('GPU num:', torch.cuda.device_count())\n",
    "    print('GPU id:', torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LeNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(LeNet, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 6, 5, padding=2)\n",
    "        self.pool1 = nn.MaxPool2d(2)\n",
    "        self.conv2 = nn.Conv2d(6, 16, 5)\n",
    "        self.pool2 = nn.MaxPool2d(2)\n",
    "        self.fc1 = nn.Linear(16*5*5, 120)\n",
    "        self.fc2 = nn.Linear(120, 84)\n",
    "        self.fc3 = nn.Linear(84, 10)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # x: (64, 1, 28, 28)\n",
    "        x = self.conv1(x)  # (64, 6, 28, 28)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool1(x)\n",
    "        \n",
    "        x = self.conv2(x)  # (64, 16, 10, 10)\n",
    "        x = F.relu(x)\n",
    "        x = self.pool2(x)\n",
    "        \n",
    "        x = x.view(x.size()[0], -1)  # (64, 256)\n",
    "        x = self.fc1(x)  # (64, 120)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.fc2(x)  # (64, 84)\n",
    "        x = F.relu(x)\n",
    "        \n",
    "        x = self.fc3(x)  # (64, 10)\n",
    "        \n",
    "        return F.log_softmax(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LeNet Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      module name  input shape output shape   params memory(MB)       MAdd      Flops  MemRead(B)  MemWrite(B) duration[%]  MemR+W(B)\n",
      "0           conv1    1  28  28    6  28  28    156.0       0.02  235,200.0  122,304.0      3760.0      18816.0      98.52%    22576.0\n",
      "1           pool1    6  28  28    6  14  14      0.0       0.00    3,528.0    4,704.0     18816.0       4704.0       0.21%    23520.0\n",
      "2           conv2    6  14  14   16  10  10   2416.0       0.01  480,000.0  241,600.0     14368.0       6400.0       0.40%    20768.0\n",
      "3           pool2   16  10  10   16   5   5      0.0       0.00    1,200.0    1,600.0      6400.0       1600.0       0.20%     8000.0\n",
      "4             fc1          400          120  48120.0       0.00   95,880.0   48,000.0    194080.0        480.0       0.30%   194560.0\n",
      "5             fc2          120           84  10164.0       0.00   20,076.0   10,080.0     41136.0        336.0       0.18%    41472.0\n",
      "6             fc3           84           10    850.0       0.00    1,670.0      840.0      3736.0         40.0       0.18%     3776.0\n",
      "total                                        61706.0       0.03  837,554.0  429,128.0      3736.0         40.0     100.00%   314672.0\n",
      "=====================================================================================================================================\n",
      "Total params: 61,706\n",
      "-------------------------------------------------------------------------------------------------------------------------------------\n",
      "Total memory: 0.03MB\n",
      "Total MAdd: 837.55KMAdd\n",
      "Total Flops: 429.13KFlops\n",
      "Total MemR+W: 307.3KB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnn = LeNet()\n",
    "stat(cnn, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[LeNet]: Use GPU\n",
      "LeNet(\n",
      "  (conv1): Conv2d(1, 6, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2))\n",
      "  (pool1): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (conv2): Conv2d(6, 16, kernel_size=(5, 5), stride=(1, 1))\n",
      "  (pool2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  (fc1): Linear(in_features=400, out_features=120, bias=True)\n",
      "  (fc2): Linear(in_features=120, out_features=84, bias=True)\n",
      "  (fc3): Linear(in_features=84, out_features=10, bias=True)\n",
      ")\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.99)\n",
      "    eps: 1e-08\n",
      "    lr: 0.001\n",
      "    weight_decay: 0\n",
      ")\n",
      "CrossEntropyLoss()\n"
     ]
    }
   ],
   "source": [
    "# 选择CNN模型\n",
    "cnn = LeNet()\n",
    "if torch.cuda.is_available():\n",
    "    cnn = cnn.cuda()\n",
    "    print('[LeNet]: Use GPU')\n",
    "else:\n",
    "    print('[LeNet]: Use CPU')\n",
    "print(cnn)\n",
    "\n",
    "# 优化器参数\n",
    "optimizer = optim.Adam(cnn.parameters(), lr=0.001, betas=(0.9, 0.99))\n",
    "print(optimizer)\n",
    "# 损失函数: 交叉熵损失函数(CE)\n",
    "criterion = nn.CrossEntropyLoss(size_average=False)\n",
    "print(criterion)\n",
    "\n",
    "# 训练CNN模型\n",
    "def train(epoch):\n",
    "    for batch_idx, (data, label) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():  # 使用GPU\n",
    "            data, label = Variable(data, volatile=True).cuda(), Variable(label, volatile=True).cuda()\n",
    "        # 梯度清零\n",
    "        optimizer.zero_grad()\n",
    "        # 模型预测结果\n",
    "        predict = cnn(data)\n",
    "        # 计算损失函数\n",
    "        loss = criterion(predict, label)\n",
    "        # 误差反向传播\n",
    "        loss.backward()\n",
    "        # 更新网络参数\n",
    "        optimizer.step()\n",
    "        # 输出训练阶段loss信息\n",
    "        if batch_idx % 200 == 0:\n",
    "            print('Train Epoch: {} [{:05d}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data.item()))\n",
    "\n",
    "# 测试CNN模型\n",
    "def test():\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, label in test_loader:\n",
    "        if torch.cuda.is_available():  # 使用GPU\n",
    "            data, label = Variable(data, volatile=True).cuda(), Variable(label, volatile=True).cuda()\n",
    "        # 模型预测结果\n",
    "        predict = cnn(data)\n",
    "        # 计算batch损失和\n",
    "        test_loss += criterion(predict, label).data.item()\n",
    "        # 预测label\n",
    "        pred = predict.data.max(1, keepdim=True)[1]\n",
    "        # 预测正确数\n",
    "        correct += pred.eq(label.data.view_as(pred)).sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    # 输出测试阶段loss信息\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        correct.item()*100.0 / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [00000/60000 (0%)]\tLoss: 146.794098\n",
      "Train Epoch: 1 [12800/60000 (21%)]\tLoss: 14.432417\n",
      "Train Epoch: 1 [25600/60000 (43%)]\tLoss: 7.218143\n",
      "Train Epoch: 1 [38400/60000 (64%)]\tLoss: 10.260456\n",
      "Train Epoch: 1 [51200/60000 (85%)]\tLoss: 2.189792\n",
      "\n",
      "Test set: Average loss: 0.0797, Accuracy: 9741/10000 (97.41%)\n",
      "\n",
      "Train Epoch: 2 [00000/60000 (0%)]\tLoss: 7.897799\n",
      "Train Epoch: 2 [12800/60000 (21%)]\tLoss: 5.142151\n",
      "Train Epoch: 2 [25600/60000 (43%)]\tLoss: 2.044629\n",
      "Train Epoch: 2 [38400/60000 (64%)]\tLoss: 1.556409\n",
      "Train Epoch: 2 [51200/60000 (85%)]\tLoss: 2.076062\n",
      "\n",
      "Test set: Average loss: 0.0417, Accuracy: 9867/10000 (98.67%)\n",
      "\n",
      "Train Epoch: 3 [00000/60000 (0%)]\tLoss: 0.760073\n",
      "Train Epoch: 3 [12800/60000 (21%)]\tLoss: 4.724016\n",
      "Train Epoch: 3 [25600/60000 (43%)]\tLoss: 0.370425\n",
      "Train Epoch: 3 [38400/60000 (64%)]\tLoss: 0.162365\n",
      "Train Epoch: 3 [51200/60000 (85%)]\tLoss: 13.794787\n",
      "\n",
      "Test set: Average loss: 0.0405, Accuracy: 9866/10000 (98.66%)\n",
      "\n",
      "Train Epoch: 4 [00000/60000 (0%)]\tLoss: 1.039356\n",
      "Train Epoch: 4 [12800/60000 (21%)]\tLoss: 0.471692\n",
      "Train Epoch: 4 [25600/60000 (43%)]\tLoss: 3.726364\n",
      "Train Epoch: 4 [38400/60000 (64%)]\tLoss: 0.726600\n",
      "Train Epoch: 4 [51200/60000 (85%)]\tLoss: 5.154440\n",
      "\n",
      "Test set: Average loss: 0.0377, Accuracy: 9863/10000 (98.63%)\n",
      "\n",
      "Train Epoch: 5 [00000/60000 (0%)]\tLoss: 0.382574\n",
      "Train Epoch: 5 [12800/60000 (21%)]\tLoss: 0.243742\n",
      "Train Epoch: 5 [25600/60000 (43%)]\tLoss: 1.578131\n",
      "Train Epoch: 5 [38400/60000 (64%)]\tLoss: 0.055124\n",
      "Train Epoch: 5 [51200/60000 (85%)]\tLoss: 0.307710\n",
      "\n",
      "Test set: Average loss: 0.0356, Accuracy: 9889/10000 (98.89%)\n",
      "\n",
      "Train Epoch: 6 [00000/60000 (0%)]\tLoss: 0.167428\n",
      "Train Epoch: 6 [12800/60000 (21%)]\tLoss: 0.682755\n",
      "Train Epoch: 6 [25600/60000 (43%)]\tLoss: 0.211075\n",
      "Train Epoch: 6 [38400/60000 (64%)]\tLoss: 0.026403\n",
      "Train Epoch: 6 [51200/60000 (85%)]\tLoss: 0.241952\n",
      "\n",
      "Test set: Average loss: 0.0294, Accuracy: 9905/10000 (99.05%)\n",
      "\n",
      "Train Epoch: 7 [00000/60000 (0%)]\tLoss: 1.885148\n",
      "Train Epoch: 7 [12800/60000 (21%)]\tLoss: 0.096396\n",
      "Train Epoch: 7 [25600/60000 (43%)]\tLoss: 1.503938\n",
      "Train Epoch: 7 [38400/60000 (64%)]\tLoss: 0.070919\n",
      "Train Epoch: 7 [51200/60000 (85%)]\tLoss: 0.440711\n",
      "\n",
      "Test set: Average loss: 0.0313, Accuracy: 9903/10000 (99.03%)\n",
      "\n",
      "Train Epoch: 8 [00000/60000 (0%)]\tLoss: 1.846126\n",
      "Train Epoch: 8 [12800/60000 (21%)]\tLoss: 7.190678\n",
      "Train Epoch: 8 [25600/60000 (43%)]\tLoss: 0.073032\n",
      "Train Epoch: 8 [38400/60000 (64%)]\tLoss: 1.029686\n",
      "Train Epoch: 8 [51200/60000 (85%)]\tLoss: 0.484207\n",
      "\n",
      "Test set: Average loss: 0.0379, Accuracy: 9879/10000 (98.79%)\n",
      "\n",
      "Train Epoch: 9 [00000/60000 (0%)]\tLoss: 0.690667\n",
      "Train Epoch: 9 [12800/60000 (21%)]\tLoss: 0.016377\n",
      "Train Epoch: 9 [25600/60000 (43%)]\tLoss: 1.387025\n",
      "Train Epoch: 9 [38400/60000 (64%)]\tLoss: 0.332814\n",
      "Train Epoch: 9 [51200/60000 (85%)]\tLoss: 0.050200\n",
      "\n",
      "Test set: Average loss: 0.0309, Accuracy: 9921/10000 (99.21%)\n",
      "\n",
      "Total Time: 40.8s.\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "for epoch in range(1, 10):\n",
    "    # 每轮训练完测试\n",
    "    train(epoch)\n",
    "    test()\n",
    "\n",
    "time_end = time.time()\n",
    "print('Total Time: {:.1f}s.'.format(time_end-time_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AlexNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 100\n",
    "learning_rate = 0.0001\n",
    "momentum = 0.9\n",
    "weight_decay = 0.0005\n",
    "k = 2\n",
    "n = 5\n",
    "alpha = 1e-4\n",
    "beta = 0.75"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 60000\n",
      "    Root location: ./\n",
      "    Split: Train\n"
     ]
    }
   ],
   "source": [
    "train_dataset = datasets.MNIST(root='./', train=True, transform=data_tf, download=False)\n",
    "print(train_dataset)\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset MNIST\n",
      "    Number of datapoints: 10000\n",
      "    Root location: ./\n",
      "    Split: Test\n"
     ]
    }
   ],
   "source": [
    "test_dataset = datasets.MNIST(root='./', train=False, transform=data_tf, download=False)\n",
    "print(test_dataset)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AlexNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(AlexNet,self).__init__()\n",
    "\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(1, 32, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "#             nn.LocalResponseNorm(n, alpha=alpha, beta=beta, k=k),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2), \n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(32, 64, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "#             nn.LocalResponseNorm(n, alpha=alpha, beta=beta, k=k),\n",
    "            nn.MaxPool2d(kernel_size=2,stride=2),\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        ) \n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, stride=1, padding=1)\n",
    "        )\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(256, 256, kernel_size=3, stride=1, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "        )\n",
    "        self.layer6 = nn.Sequential(\n",
    "            nn.Linear(256*3*3, 1024),\n",
    "            nn.Dropout(),\n",
    "            nn.ReLU(inplace=True)\n",
    "        )\n",
    "        self.layer7 = nn.Sequential(\n",
    "            nn.Linear(1024, 512),\n",
    "            nn.Dropout(),\n",
    "            nn.ReLU(inplace=True)\n",
    "        ) \n",
    "        self.layer8 = nn.Sequential(\n",
    "            nn.Linear(512, 10)\n",
    "        )\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.layer5(self.layer4(self.layer3(self.layer2(self.layer1(x)))))\n",
    "        x = x.view(-1, 256*3*3)\n",
    "        x = self.layer8(self.layer7(self.layer6(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AlexNet Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MAdd]: Dropout is not supported!\n",
      "[Flops]: Dropout is not supported!\n",
      "[Memory]: Dropout is not supported!\n",
      "[MAdd]: Dropout is not supported!\n",
      "[Flops]: Dropout is not supported!\n",
      "[Memory]: Dropout is not supported!\n",
      "[MAdd]: Dropout is not supported!\n",
      "[Flops]: Dropout is not supported!\n",
      "[Memory]: Dropout is not supported!\n",
      "[MAdd]: Dropout is not supported!\n",
      "[Flops]: Dropout is not supported!\n",
      "[Memory]: Dropout is not supported!\n",
      "[MAdd]: Dropout is not supported!\n",
      "[Flops]: Dropout is not supported!\n",
      "[Memory]: Dropout is not supported!\n",
      "[MAdd]: Dropout is not supported!\n",
      "[Flops]: Dropout is not supported!\n",
      "[Memory]: Dropout is not supported!\n",
      "[MAdd]: Dropout is not supported!\n",
      "[Flops]: Dropout is not supported!\n",
      "[Memory]: Dropout is not supported!\n",
      "[MAdd]: Dropout is not supported!\n",
      "[Flops]: Dropout is not supported!\n",
      "[Memory]: Dropout is not supported!\n",
      "      module name  input shape output shape     params memory(MB)           MAdd         Flops  MemRead(B)  MemWrite(B) duration[%]   MemR+W(B)\n",
      "0        layer1.0    1  28  28   32  28  28      320.0       0.10      451,584.0     250,880.0      4416.0     100352.0       8.44%    104768.0\n",
      "1        layer1.1   32  28  28   32  28  28        0.0       0.10       25,088.0      25,088.0    100352.0     100352.0       2.44%    200704.0\n",
      "2        layer1.2   32  28  28   32  14  14        0.0       0.02       18,816.0      25,088.0    100352.0      25088.0       4.05%    125440.0\n",
      "3        layer2.0   32  14  14   64  14  14    18496.0       0.05    7,225,344.0   3,625,216.0     99072.0      50176.0      10.08%    149248.0\n",
      "4        layer2.1   64  14  14   64  14  14        0.0       0.05       12,544.0      12,544.0     50176.0      50176.0       2.19%    100352.0\n",
      "5        layer2.2   64  14  14   64   7   7        0.0       0.01        9,408.0      12,544.0     50176.0      12544.0       3.75%     62720.0\n",
      "6        layer3.0   64   7   7  128   7   7    73856.0       0.02    7,225,344.0   3,618,944.0    307968.0      25088.0       8.29%    333056.0\n",
      "7        layer4.0  128   7   7  256   7   7   295168.0       0.05   28,901,376.0  14,463,232.0   1205760.0      50176.0      11.33%   1255936.0\n",
      "8        layer5.0  256   7   7  256   7   7   590080.0       0.05   57,802,752.0  28,913,920.0   2410496.0      50176.0      12.32%   2460672.0\n",
      "9        layer5.1  256   7   7  256   7   7        0.0       0.05       12,544.0      12,544.0     50176.0      50176.0       2.59%    100352.0\n",
      "10       layer5.2  256   7   7  256   3   3        0.0       0.01        6,912.0      12,544.0     50176.0       9216.0       4.09%     59392.0\n",
      "11       layer6.0         2304         1024  2360320.0       0.00    4,717,568.0   2,359,296.0   9450496.0       4096.0       7.16%   9454592.0\n",
      "12       layer6.1         1024         1024        0.0       0.00            0.0           0.0         0.0          0.0       6.82%         0.0\n",
      "13       layer6.2         1024         1024        0.0       0.00        1,024.0       1,024.0      4096.0       4096.0       2.37%      8192.0\n",
      "14       layer7.0         1024          512   524800.0       0.00    1,048,064.0     524,288.0   2103296.0       2048.0       4.16%   2105344.0\n",
      "15       layer7.1          512          512        0.0       0.00            0.0           0.0         0.0          0.0       4.16%         0.0\n",
      "16       layer7.2          512          512        0.0       0.00          512.0         512.0      2048.0       2048.0       2.61%      4096.0\n",
      "17       layer8.0          512           10     5130.0       0.00       10,230.0       5,120.0     22568.0         40.0       3.15%     22608.0\n",
      "total                                        3868170.0       0.52  107,469,110.0  53,862,784.0     22568.0         40.0     100.00%  16547472.0\n",
      "===============================================================================================================================================\n",
      "Total params: 3,868,170\n",
      "-----------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Total memory: 0.52MB\n",
      "Total MAdd: 107.47MMAdd\n",
      "Total Flops: 53.86MFlops\n",
      "Total MemR+W: 15.78MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnn = AlexNet()\n",
    "stat(cnn, (1, 28, 28))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[AlexNet]: Use GPU\n",
      "AlexNet(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "  )\n",
      "  (layer5): Sequential(\n",
      "    (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer6): Sequential(\n",
      "    (0): Linear(in_features=2304, out_features=1024, bias=True)\n",
      "    (1): Dropout(p=0.5)\n",
      "    (2): ReLU(inplace)\n",
      "  )\n",
      "  (layer7): Sequential(\n",
      "    (0): Linear(in_features=1024, out_features=512, bias=True)\n",
      "    (1): Dropout(p=0.5)\n",
      "    (2): ReLU(inplace)\n",
      "  )\n",
      "  (layer8): Sequential(\n",
      "    (0): Linear(in_features=512, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.0001\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 0.0005\n",
      ")\n",
      "CrossEntropyLoss()\n"
     ]
    }
   ],
   "source": [
    "# 选择CNN模型\n",
    "cnn = AlexNet()\n",
    "if torch.cuda.is_available():\n",
    "    cnn = cnn.cuda()\n",
    "    print('[AlexNet]: Use GPU')\n",
    "else:\n",
    "    print('[AlexNet]: Use CPU')\n",
    "print(cnn)\n",
    "\n",
    "\n",
    "# 优化器参数\n",
    "optimizer = optim.SGD(cnn.parameters(), lr=0.0001, momentum=momentum, weight_decay=weight_decay)\n",
    "# optimizer = optim.Adam(cnn.parameters(), lr=0.001, betas=(0.9, 0.99))\n",
    "print(optimizer)\n",
    "# 损失函数: 交叉熵损失函数(CE)\n",
    "criterion = nn.CrossEntropyLoss(size_average=False)\n",
    "print(criterion)\n",
    "\n",
    "# 训练CNN模型\n",
    "def train(epoch):\n",
    "    for batch_idx, (data, label) in enumerate(train_loader):\n",
    "        if torch.cuda.is_available():  # 使用GPU\n",
    "            data, label = Variable(data, volatile=True).cuda(), Variable(label, volatile=True).cuda()\n",
    "        # 梯度清零\n",
    "        optimizer.zero_grad()\n",
    "        # 模型预测结果\n",
    "        predict = cnn(data)\n",
    "        # 计算损失函数\n",
    "        loss = criterion(predict, label)\n",
    "        # 误差反向传播\n",
    "        loss.backward()\n",
    "        # 更新网络参数\n",
    "        optimizer.step()\n",
    "        # 输出训练阶段loss信息\n",
    "        if batch_idx % 100 == 0:\n",
    "            print('Train Epoch: {} [{:05d}/{} ({:.0f}%)]\\tLoss: {:.6f}'.format(\n",
    "                epoch, batch_idx * len(data), len(train_loader.dataset),\n",
    "                100. * batch_idx / len(train_loader), loss.data.item()))\n",
    "\n",
    "# 测试CNN模型\n",
    "def test():\n",
    "    test_loss = 0\n",
    "    correct = 0\n",
    "    for data, label in test_loader:\n",
    "        if torch.cuda.is_available():  # 使用GPU\n",
    "            data, label = Variable(data, volatile=True).cuda(), Variable(label, volatile=True).cuda()\n",
    "        # 模型预测结果\n",
    "        predict = cnn(data)\n",
    "        # 计算batch损失和\n",
    "        test_loss += criterion(predict, label).data.item()\n",
    "        # 预测label\n",
    "        pred = predict.data.max(1, keepdim=True)[1]\n",
    "        # 预测正确数\n",
    "        correct += pred.eq(label.data.view_as(pred)).sum()\n",
    "\n",
    "    test_loss /= len(test_loader.dataset)\n",
    "    # 输出测试阶段loss信息\n",
    "    print('\\nTest set: Average loss: {:.4f}, Accuracy: {}/{} ({:.2f}%)\\n'.format(\n",
    "        test_loss, correct, len(test_loader.dataset),\n",
    "        correct.item()*100.0 / len(test_loader.dataset)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [00000/60000 (0%)]\tLoss: 230.157242\n",
      "Train Epoch: 1 [20000/60000 (33%)]\tLoss: 220.307007\n",
      "Train Epoch: 1 [40000/60000 (67%)]\tLoss: 9.427719\n",
      "\n",
      "Test set: Average loss: 0.1058, Accuracy: 9662/10000 (96.62%)\n",
      "\n",
      "Train Epoch: 2 [00000/60000 (0%)]\tLoss: 18.087711\n",
      "Train Epoch: 2 [20000/60000 (33%)]\tLoss: 5.371624\n",
      "Train Epoch: 2 [40000/60000 (67%)]\tLoss: 5.955410\n",
      "\n",
      "Test set: Average loss: 0.0427, Accuracy: 9849/10000 (98.49%)\n",
      "\n",
      "Train Epoch: 3 [00000/60000 (0%)]\tLoss: 1.183239\n",
      "Train Epoch: 3 [20000/60000 (33%)]\tLoss: 6.469438\n",
      "Train Epoch: 3 [40000/60000 (67%)]\tLoss: 16.949844\n",
      "\n",
      "Test set: Average loss: 0.0312, Accuracy: 9891/10000 (98.91%)\n",
      "\n",
      "Train Epoch: 4 [00000/60000 (0%)]\tLoss: 0.848212\n",
      "Train Epoch: 4 [20000/60000 (33%)]\tLoss: 6.409442\n",
      "Train Epoch: 4 [40000/60000 (67%)]\tLoss: 1.325053\n",
      "\n",
      "Test set: Average loss: 0.0327, Accuracy: 9893/10000 (98.93%)\n",
      "\n",
      "Train Epoch: 5 [00000/60000 (0%)]\tLoss: 0.671915\n",
      "Train Epoch: 5 [20000/60000 (33%)]\tLoss: 7.048769\n",
      "Train Epoch: 5 [40000/60000 (67%)]\tLoss: 10.748867\n",
      "\n",
      "Test set: Average loss: 0.0337, Accuracy: 9888/10000 (98.88%)\n",
      "\n",
      "Train Epoch: 6 [00000/60000 (0%)]\tLoss: 4.289066\n",
      "Train Epoch: 6 [20000/60000 (33%)]\tLoss: 0.553534\n",
      "Train Epoch: 6 [40000/60000 (67%)]\tLoss: 0.083187\n",
      "\n",
      "Test set: Average loss: 0.0248, Accuracy: 9915/10000 (99.15%)\n",
      "\n",
      "Train Epoch: 7 [00000/60000 (0%)]\tLoss: 6.944621\n",
      "Train Epoch: 7 [20000/60000 (33%)]\tLoss: 3.742116\n",
      "Train Epoch: 7 [40000/60000 (67%)]\tLoss: 0.690495\n",
      "\n",
      "Test set: Average loss: 0.0286, Accuracy: 9904/10000 (99.04%)\n",
      "\n",
      "Train Epoch: 8 [00000/60000 (0%)]\tLoss: 4.164841\n",
      "Train Epoch: 8 [20000/60000 (33%)]\tLoss: 0.590837\n",
      "Train Epoch: 8 [40000/60000 (67%)]\tLoss: 1.725040\n",
      "\n",
      "Test set: Average loss: 0.0271, Accuracy: 9911/10000 (99.11%)\n",
      "\n",
      "Train Epoch: 9 [00000/60000 (0%)]\tLoss: 1.079371\n",
      "Train Epoch: 9 [20000/60000 (33%)]\tLoss: 2.086443\n",
      "Train Epoch: 9 [40000/60000 (67%)]\tLoss: 0.260547\n",
      "\n",
      "Test set: Average loss: 0.0214, Accuracy: 9931/10000 (99.31%)\n",
      "\n",
      "Total Time: 96.2s.\n"
     ]
    }
   ],
   "source": [
    "time_start = time.time()\n",
    "for epoch in range(1, 10):\n",
    "    # 每轮训练完测试\n",
    "    train(epoch)\n",
    "    test()\n",
    "\n",
    "time_end = time.time()\n",
    "print('Total Time: {:.1f}s.'.format(time_end-time_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG-16"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 256\n",
    "learning_rate = 0.01\n",
    "momentum = 0.9\n",
    "weight_decay = 0.0005"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG16(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(VGG16, self).__init__()    # (N, 3, 224, 224)\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),    # (N, 64, 224, 224)\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),    # (N, 64, 224, 224)\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),    # (N, 64, 112, 112)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),    # (N, 128, 112, 112)\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),    # (N, 128, 112, 112)\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),    # (N, 128, 56, 56)\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),    # (N, 256, 56, 56)\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),    # (N, 256, 56, 56)\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),    # (N, 256, 56, 56)\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),    # (N, 256, 28, 28)          \n",
    "        )\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),    # (N, 512, 28, 28)\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),    # (N, 512, 28, 28)\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),    # (N, 512, 28, 28)\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)        \n",
    "        )\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),    # (N, 512, 14, 14)\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),    # (N, 512, 14, 14)\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),    # (N, 512, 14, 14)\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)    # (N, 512, 7, 7)      \n",
    "        )\n",
    "        \n",
    "        self.layer6 = nn.Sequential(\n",
    "            nn.Linear(512*7*7, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout()\n",
    "        )\n",
    "        self.layer7 = nn.Sequential(\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout()     \n",
    "        )     \n",
    "        self.layer8 = nn.Sequential(\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer5(self.layer4(self.layer3(self.layer2(self.layer1(x)))))\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.layer8(self.layer7(self.layer6(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG-16 Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MAdd]: Dropout is not supported!\n",
      "[Flops]: Dropout is not supported!\n",
      "[Memory]: Dropout is not supported!\n",
      "[MAdd]: Dropout is not supported!\n",
      "[Flops]: Dropout is not supported!\n",
      "[Memory]: Dropout is not supported!\n",
      "      module name  input shape output shape       params memory(MB)              MAdd             Flops   MemRead(B)  MemWrite(B) duration[%]    MemR+W(B)\n",
      "0        layer1.0    3 224 224   64 224 224       1792.0      12.25     173,408,256.0      89,915,392.0     609280.0   12845056.0       2.65%   13454336.0\n",
      "1        layer1.1   64 224 224   64 224 224          0.0      12.25       3,211,264.0       3,211,264.0   12845056.0   12845056.0       0.51%   25690112.0\n",
      "2        layer1.2   64 224 224   64 224 224      36928.0      12.25   3,699,376,128.0   1,852,899,328.0   12992768.0   12845056.0       8.30%   25837824.0\n",
      "3        layer1.3   64 224 224   64 224 224          0.0      12.25       3,211,264.0       3,211,264.0   12845056.0   12845056.0       0.43%   25690112.0\n",
      "4        layer1.4   64 224 224   64 112 112          0.0       3.06       2,408,448.0       3,211,264.0   12845056.0    3211264.0       7.37%   16056320.0\n",
      "5        layer2.0   64 112 112  128 112 112      73856.0       6.12   1,849,688,064.0     926,449,664.0    3506688.0    6422528.0       3.95%    9929216.0\n",
      "6        layer2.1  128 112 112  128 112 112          0.0       6.12       1,605,632.0       1,605,632.0    6422528.0    6422528.0       0.21%   12845056.0\n",
      "7        layer2.2  128 112 112  128 112 112     147584.0       6.12   3,699,376,128.0   1,851,293,696.0    7012864.0    6422528.0       8.06%   13435392.0\n",
      "8        layer2.3  128 112 112  128 112 112          0.0       6.12       1,605,632.0       1,605,632.0    6422528.0    6422528.0       0.22%   12845056.0\n",
      "9        layer2.4  128 112 112  128  56  56          0.0       1.53       1,204,224.0       1,605,632.0    6422528.0    1605632.0       2.20%    8028160.0\n",
      "10       layer3.0  128  56  56  256  56  56     295168.0       3.06   1,849,688,064.0     925,646,848.0    2786304.0    3211264.0       2.97%    5997568.0\n",
      "11       layer3.1  256  56  56  256  56  56          0.0       3.06         802,816.0         802,816.0    3211264.0    3211264.0       0.12%    6422528.0\n",
      "12       layer3.2  256  56  56  256  56  56     590080.0       3.06   3,699,376,128.0   1,850,490,880.0    5571584.0    3211264.0       6.51%    8782848.0\n",
      "13       layer3.3  256  56  56  256  56  56          0.0       3.06         802,816.0         802,816.0    3211264.0    3211264.0       0.06%    6422528.0\n",
      "14       layer3.4  256  56  56  256  56  56     590080.0       3.06   3,699,376,128.0   1,850,490,880.0    5571584.0    3211264.0       6.65%    8782848.0\n",
      "15       layer3.5  256  56  56  256  56  56          0.0       3.06         802,816.0         802,816.0    3211264.0    3211264.0       0.09%    6422528.0\n",
      "16       layer3.6  256  56  56  256  28  28          0.0       0.77         602,112.0         802,816.0    3211264.0     802816.0       1.04%    4014080.0\n",
      "17       layer4.0  256  28  28  512  28  28    1180160.0       1.53   1,849,688,064.0     925,245,440.0    5523456.0    1605632.0       3.58%    7129088.0\n",
      "18       layer4.1  512  28  28  512  28  28          0.0       1.53         401,408.0         401,408.0    1605632.0    1605632.0       0.04%    3211264.0\n",
      "19       layer4.2  512  28  28  512  28  28    2359808.0       1.53   3,699,376,128.0   1,850,089,472.0   11044864.0    1605632.0       7.34%   12650496.0\n",
      "20       layer4.3  512  28  28  512  28  28          0.0       1.53         401,408.0         401,408.0    1605632.0    1605632.0       0.04%    3211264.0\n",
      "21       layer4.4  512  28  28  512  28  28    2359808.0       1.53   3,699,376,128.0   1,850,089,472.0   11044864.0    1605632.0       6.43%   12650496.0\n",
      "22       layer4.5  512  28  28  512  28  28          0.0       1.53         401,408.0         401,408.0    1605632.0    1605632.0       0.04%    3211264.0\n",
      "23       layer4.6  512  28  28  512  14  14          0.0       0.38         301,056.0         401,408.0    1605632.0     401408.0       0.60%    2007040.0\n",
      "24       layer5.0  512  14  14  512  14  14    2359808.0       0.38     924,844,032.0     462,522,368.0    9840640.0     401408.0       2.06%   10242048.0\n",
      "25       layer5.1  512  14  14  512  14  14          0.0       0.38         100,352.0         100,352.0     401408.0     401408.0       0.02%     802816.0\n",
      "26       layer5.2  512  14  14  512  14  14    2359808.0       0.38     924,844,032.0     462,522,368.0    9840640.0     401408.0       1.94%   10242048.0\n",
      "27       layer5.3  512  14  14  512  14  14          0.0       0.38         100,352.0         100,352.0     401408.0     401408.0       0.18%     802816.0\n",
      "28       layer5.4  512  14  14  512  14  14    2359808.0       0.38     924,844,032.0     462,522,368.0    9840640.0     401408.0       2.11%   10242048.0\n",
      "29       layer5.5  512  14  14  512  14  14          0.0       0.38         100,352.0         100,352.0     401408.0     401408.0       0.17%     802816.0\n",
      "30       layer5.6  512  14  14  512   7   7          0.0       0.10          75,264.0         100,352.0     401408.0     100352.0       0.16%     501760.0\n",
      "31       layer6.0        25088         4096  102764544.0       0.02     205,516,800.0     102,760,448.0  411158528.0      16384.0      22.39%  411174912.0\n",
      "32       layer6.1         4096         4096          0.0       0.02           4,096.0           4,096.0      16384.0      16384.0       0.02%      32768.0\n",
      "33       layer6.2         4096         4096          0.0       0.02               0.0               0.0          0.0          0.0       0.02%          0.0\n",
      "34       layer7.0         4096         4096   16781312.0       0.02      33,550,336.0      16,777,216.0   67141632.0      16384.0       1.31%   67158016.0\n",
      "35       layer7.1         4096         4096          0.0       0.02           4,096.0           4,096.0      16384.0      16384.0       0.01%      32768.0\n",
      "36       layer7.2         4096         4096          0.0       0.02               0.0               0.0          0.0          0.0       0.01%          0.0\n",
      "37       layer8.0         4096           10      40970.0       0.00          81,910.0          40,960.0     180264.0         40.0       0.19%     180304.0\n",
      "total                                        134301514.0     109.29  30,950,557,174.0  15,499,433,984.0     180264.0         40.0     100.00%  766942544.0\n",
      "==========================================================================================================================================================\n",
      "Total params: 134,301,514\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Total memory: 109.29MB\n",
      "Total MAdd: 30.95GMAdd\n",
      "Total Flops: 15.5GFlops\n",
      "Total MemR+W: 731.41MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnn = VGG16(10)\n",
    "stat(cnn, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VGG-16]: Use GPU\n",
      "VGG16(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace)\n",
      "    (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): ReLU(inplace)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace)\n",
      "    (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): ReLU(inplace)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer5): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace)\n",
      "    (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): ReLU(inplace)\n",
      "    (6): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer6): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Dropout(p=0.5)\n",
      "  )\n",
      "  (layer7): Sequential(\n",
      "    (0): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Dropout(p=0.5)\n",
      "  )\n",
      "  (layer8): Sequential(\n",
      "    (0): Linear(in_features=4096, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.0001\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 0.0005\n",
      ")\n",
      "CrossEntropyLoss()\n"
     ]
    }
   ],
   "source": [
    "# 选择CNN模型\n",
    "cnn = VGG16(10)\n",
    "if torch.cuda.is_available():\n",
    "    cnn = cnn.cuda()\n",
    "    print('[VGG-16]: Use GPU')\n",
    "else:\n",
    "    print('[VGG-16]: Use CPU')\n",
    "print(cnn)\n",
    "\n",
    "\n",
    "# 优化器参数\n",
    "optimizer = optim.SGD(cnn.parameters(), lr=0.0001, momentum=momentum, weight_decay=weight_decay)\n",
    "# optimizer = optim.Adam(cnn.parameters(), lr=0.001, betas=(0.9, 0.99))\n",
    "print(optimizer)\n",
    "# 损失函数: 交叉熵损失函数(CE)\n",
    "criterion = nn.CrossEntropyLoss(size_average=False)\n",
    "print(criterion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_start = time.time()\n",
    "for epoch in range(1, 10):\n",
    "    # 每轮训练完测试\n",
    "    train(epoch)\n",
    "    test()\n",
    "\n",
    "time_end = time.time()\n",
    "print('Total Time: {:.1f}s.'.format(time_end-time_start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## VGG-19"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VGG19(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(VGG19, self).__init__()    # (N, 3, 224, 224)\n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, kernel_size=3, padding=1),    # (N, 64, 224, 224)\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),    # (N, 64, 224, 224)\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),    # (N, 64, 112, 112)\n",
    "        )\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),    # (N, 128, 112, 112)\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),    # (N, 128, 112, 112)\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),    # (N, 128, 56, 56)\n",
    "        )\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),    # (N, 256, 56, 56)\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),    # (N, 256, 56, 56)\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),    # (N, 256, 56, 56)\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),    # (N, 256, 56, 56)\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),    # (N, 256, 28, 28)          \n",
    "        )\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),    # (N, 512, 28, 28)\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),    # (N, 512, 28, 28)\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),    # (N, 512, 28, 28)\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),    # (N, 512, 28, 28)\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)        \n",
    "        )\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),    # (N, 512, 14, 14)\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),    # (N, 512, 14, 14)\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),    # (N, 512, 14, 14)\n",
    "            nn.ReLU(True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),    # (N, 512, 14, 14)\n",
    "            nn.ReLU(True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)    # (N, 512, 7, 7)      \n",
    "        )\n",
    "        \n",
    "        self.layer6 = nn.Sequential(\n",
    "            nn.Linear(512*7*7, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout()\n",
    "        )\n",
    "        self.layer7 = nn.Sequential(\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(True),\n",
    "            nn.Dropout()     \n",
    "        )     \n",
    "        self.layer8 = nn.Sequential(\n",
    "            nn.Linear(4096, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.layer5(self.layer4(self.layer3(self.layer2(self.layer1(x)))))\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        x = self.layer8(self.layer7(self.layer6(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VGG-19 Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[MAdd]: Dropout is not supported!\n",
      "[Flops]: Dropout is not supported!\n",
      "[Memory]: Dropout is not supported!\n",
      "[MAdd]: Dropout is not supported!\n",
      "[Flops]: Dropout is not supported!\n",
      "[Memory]: Dropout is not supported!\n",
      "[MAdd]: Dropout is not supported!\n",
      "[Flops]: Dropout is not supported!\n",
      "[Memory]: Dropout is not supported!\n",
      "[MAdd]: Dropout is not supported!\n",
      "[Flops]: Dropout is not supported!\n",
      "[Memory]: Dropout is not supported!\n",
      "      module name  input shape output shape       params memory(MB)              MAdd             Flops   MemRead(B)  MemWrite(B) duration[%]    MemR+W(B)\n",
      "0        layer1.0    3 224 224   64 224 224       1792.0      12.25     173,408,256.0      89,915,392.0     609280.0   12845056.0       2.43%   13454336.0\n",
      "1        layer1.1   64 224 224   64 224 224          0.0      12.25       3,211,264.0       3,211,264.0   12845056.0   12845056.0       0.42%   25690112.0\n",
      "2        layer1.2   64 224 224   64 224 224      36928.0      12.25   3,699,376,128.0   1,852,899,328.0   12992768.0   12845056.0       8.10%   25837824.0\n",
      "3        layer1.3   64 224 224   64 224 224          0.0      12.25       3,211,264.0       3,211,264.0   12845056.0   12845056.0       0.50%   25690112.0\n",
      "4        layer1.4   64 224 224   64 112 112          0.0       3.06       2,408,448.0       3,211,264.0   12845056.0    3211264.0       5.51%   16056320.0\n",
      "5        layer2.0   64 112 112  128 112 112      73856.0       6.12   1,849,688,064.0     926,449,664.0    3506688.0    6422528.0       3.74%    9929216.0\n",
      "6        layer2.1  128 112 112  128 112 112          0.0       6.12       1,605,632.0       1,605,632.0    6422528.0    6422528.0       0.19%   12845056.0\n",
      "7        layer2.2  128 112 112  128 112 112     147584.0       6.12   3,699,376,128.0   1,851,293,696.0    7012864.0    6422528.0       7.29%   13435392.0\n",
      "8        layer2.3  128 112 112  128 112 112          0.0       6.12       1,605,632.0       1,605,632.0    6422528.0    6422528.0       0.18%   12845056.0\n",
      "9        layer2.4  128 112 112  128  56  56          0.0       1.53       1,204,224.0       1,605,632.0    6422528.0    1605632.0       2.17%    8028160.0\n",
      "10       layer3.0  128  56  56  256  56  56     295168.0       3.06   1,849,688,064.0     925,646,848.0    2786304.0    3211264.0       3.33%    5997568.0\n",
      "11       layer3.1  256  56  56  256  56  56          0.0       3.06         802,816.0         802,816.0    3211264.0    3211264.0       0.23%    6422528.0\n",
      "12       layer3.2  256  56  56  256  56  56     590080.0       3.06   3,699,376,128.0   1,850,490,880.0    5571584.0    3211264.0       6.02%    8782848.0\n",
      "13       layer3.3  256  56  56  256  56  56          0.0       3.06         802,816.0         802,816.0    3211264.0    3211264.0       0.14%    6422528.0\n",
      "14       layer3.4  256  56  56  256  56  56     590080.0       3.06   3,699,376,128.0   1,850,490,880.0    5571584.0    3211264.0       6.15%    8782848.0\n",
      "15       layer3.5  256  56  56  256  56  56          0.0       3.06         802,816.0         802,816.0    3211264.0    3211264.0       0.10%    6422528.0\n",
      "16       layer3.6  256  56  56  256  56  56     590080.0       3.06   3,699,376,128.0   1,850,490,880.0    5571584.0    3211264.0       6.97%    8782848.0\n",
      "17       layer3.7  256  56  56  256  56  56          0.0       3.06         802,816.0         802,816.0    3211264.0    3211264.0       0.26%    6422528.0\n",
      "18       layer3.8  256  56  56  256  28  28          0.0       0.77         602,112.0         802,816.0    3211264.0     802816.0       1.04%    4014080.0\n",
      "19       layer4.0  256  28  28  512  28  28    1180160.0       1.53   1,849,688,064.0     925,245,440.0    5523456.0    1605632.0       3.35%    7129088.0\n",
      "20       layer4.1  512  28  28  512  28  28          0.0       1.53         401,408.0         401,408.0    1605632.0    1605632.0       0.05%    3211264.0\n",
      "21       layer4.2  512  28  28  512  28  28    2359808.0       1.53   3,699,376,128.0   1,850,089,472.0   11044864.0    1605632.0       6.70%   12650496.0\n",
      "22       layer4.3  512  28  28  512  28  28          0.0       1.53         401,408.0         401,408.0    1605632.0    1605632.0       0.08%    3211264.0\n",
      "23       layer4.4  512  28  28  512  28  28    2359808.0       1.53   3,699,376,128.0   1,850,089,472.0   11044864.0    1605632.0       7.08%   12650496.0\n",
      "24       layer4.5  512  28  28  512  28  28          0.0       1.53         401,408.0         401,408.0    1605632.0    1605632.0       0.07%    3211264.0\n",
      "25       layer4.6  512  28  28  512  28  28    2359808.0       1.53   3,699,376,128.0   1,850,089,472.0   11044864.0    1605632.0       6.82%   12650496.0\n",
      "26       layer4.7  512  28  28  512  28  28          0.0       1.53         401,408.0         401,408.0    1605632.0    1605632.0       0.07%    3211264.0\n",
      "27       layer4.8  512  28  28  512  14  14          0.0       0.38         301,056.0         401,408.0    1605632.0     401408.0       0.62%    2007040.0\n",
      "28       layer5.0  512  14  14  512  14  14    2359808.0       0.38     924,844,032.0     462,522,368.0    9840640.0     401408.0       2.66%   10242048.0\n",
      "29       layer5.1  512  14  14  512  14  14          0.0       0.38         100,352.0         100,352.0     401408.0     401408.0       0.16%     802816.0\n",
      "30       layer5.2  512  14  14  512  14  14    2359808.0       0.38     924,844,032.0     462,522,368.0    9840640.0     401408.0       2.42%   10242048.0\n",
      "31       layer5.3  512  14  14  512  14  14          0.0       0.38         100,352.0         100,352.0     401408.0     401408.0       0.06%     802816.0\n",
      "32       layer5.4  512  14  14  512  14  14    2359808.0       0.38     924,844,032.0     462,522,368.0    9840640.0     401408.0       3.07%   10242048.0\n",
      "33       layer5.5  512  14  14  512  14  14          0.0       0.38         100,352.0         100,352.0     401408.0     401408.0       0.06%     802816.0\n",
      "34       layer5.6  512  14  14  512  14  14    2359808.0       0.38     924,844,032.0     462,522,368.0    9840640.0     401408.0       3.10%   10242048.0\n",
      "35       layer5.7  512  14  14  512  14  14          0.0       0.38         100,352.0         100,352.0     401408.0     401408.0       0.05%     802816.0\n",
      "36       layer5.8  512  14  14  512   7   7          0.0       0.10          75,264.0         100,352.0     401408.0     100352.0       0.19%     501760.0\n",
      "37       layer6.0        25088         4096  102764544.0       0.02     205,516,800.0     102,760,448.0  411158528.0      16384.0       7.11%  411174912.0\n",
      "38       layer6.1         4096         4096          0.0       0.02           4,096.0           4,096.0      16384.0      16384.0       0.04%      32768.0\n",
      "39       layer6.2         4096         4096          0.0       0.02               0.0               0.0          0.0          0.0       0.06%          0.0\n",
      "40       layer7.0         4096         4096   16781312.0       0.02      33,550,336.0      16,777,216.0   67141632.0      16384.0       1.04%   67158016.0\n",
      "41       layer7.1         4096         4096          0.0       0.02           4,096.0           4,096.0      16384.0      16384.0       0.04%      32768.0\n",
      "42       layer7.2         4096         4096          0.0       0.02               0.0               0.0          0.0          0.0       0.07%          0.0\n",
      "43       layer8.0         4096           10      40970.0       0.00          81,910.0          40,960.0     180264.0         40.0       0.27%     180304.0\n",
      "total                                        139611210.0     119.24  39,275,458,038.0  19,663,841,280.0     180264.0         40.0     100.00%  809054544.0\n",
      "==========================================================================================================================================================\n",
      "Total params: 139,611,210\n",
      "----------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Total memory: 119.24MB\n",
      "Total MAdd: 39.28GMAdd\n",
      "Total Flops: 19.66GFlops\n",
      "Total MemR+W: 771.57MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnn = VGG19(10)\n",
    "stat(cnn, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[VGG-19]: Use GPU\n",
      "VGG19(\n",
      "  (layer1): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace)\n",
      "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace)\n",
      "    (4): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): ReLU(inplace)\n",
      "    (6): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace)\n",
      "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace)\n",
      "    (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): ReLU(inplace)\n",
      "    (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace)\n",
      "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer5): Sequential(\n",
      "    (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (3): ReLU(inplace)\n",
      "    (4): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (5): ReLU(inplace)\n",
      "    (6): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
      "    (7): ReLU(inplace)\n",
      "    (8): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer6): Sequential(\n",
      "    (0): Linear(in_features=25088, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Dropout(p=0.5)\n",
      "  )\n",
      "  (layer7): Sequential(\n",
      "    (0): Linear(in_features=4096, out_features=4096, bias=True)\n",
      "    (1): ReLU(inplace)\n",
      "    (2): Dropout(p=0.5)\n",
      "  )\n",
      "  (layer8): Sequential(\n",
      "    (0): Linear(in_features=4096, out_features=10, bias=True)\n",
      "  )\n",
      ")\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.0001\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 0.0005\n",
      ")\n",
      "CrossEntropyLoss()\n"
     ]
    }
   ],
   "source": [
    "# 选择CNN模型\n",
    "cnn = VGG19(10)\n",
    "if torch.cuda.is_available():\n",
    "    cnn = cnn.cuda()\n",
    "    print('[VGG-19]: Use GPU')\n",
    "else:\n",
    "    print('[VGG-19]: Use CPU')\n",
    "print(cnn)\n",
    "\n",
    "\n",
    "# 优化器参数\n",
    "optimizer = optim.SGD(cnn.parameters(), lr=0.0001, momentum=momentum, weight_decay=weight_decay)\n",
    "# optimizer = optim.Adam(cnn.parameters(), lr=0.001, betas=(0.9, 0.99))\n",
    "print(optimizer)\n",
    "# 损失函数: 交叉熵损失函数(CE)\n",
    "criterion = nn.CrossEntropyLoss(size_average=False)\n",
    "print(criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## GoogLeNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BasicConv(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, **kwargs):\n",
    "        super(BasicConv, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, out_channels, bias=False, **kwargs)\n",
    "        self.bn = nn.BatchNorm2d(out_channels, eps=0.001)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.conv(x)\n",
    "        x = self.bn(x)\n",
    "        return x\n",
    "    \n",
    "class Inception(nn.Module):\n",
    "    def __init__(self, in_channels):\n",
    "        super(Inception, self).__init__()\n",
    "        self.module_1x1 = nn.Sequential(\n",
    "            BasicConv(in_channels, 64, kernel_size=1)\n",
    "        )\n",
    "        self.module_3x3 = nn.Sequential(\n",
    "            BasicConv(in_channels, 64, kernel_size=1),\n",
    "            BasicConv(64, 96, kernel_size=3, padding=1),\n",
    "            BasicConv(96, 96, kernel_size=3, padding=1)\n",
    "        ) \n",
    "        self.module_5x5 = nn.Sequential(\n",
    "            BasicConv(in_channels, 48, kernel_size=1),\n",
    "            BasicConv(48, 64, kernel_size=5, padding=2)\n",
    "        )\n",
    "        self.module_pool = nn.Sequential(\n",
    "            nn.MaxPool2d(kernel_size=3, padding=1),\n",
    "            BasicConv(in_channels, 64, kernel_size=1, padding=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x1 = self.module_1x1(x)\n",
    "        print('x1:', x1.shape)\n",
    "        x2 = self.module_3x3(x)\n",
    "        print('x2:', x2.shape)\n",
    "        x3 = self.module_5x5(x)\n",
    "        print('x3:', x3.shape)\n",
    "        x4 = self.module_pool(x)\n",
    "        print('x4:', x4.shape)\n",
    "        out = [x1, x2, x3, x4]\n",
    "        return torch.cat(out, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cnn = Inception(3)\n",
    "stat(cnn, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Inception]: Use GPU\n",
      "Inception(\n",
      "  (module_1x1): Sequential(\n",
      "    (0): BasicConv(\n",
      "      (conv): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (module_3x3): Sequential(\n",
      "    (0): BasicConv(\n",
      "      (conv): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicConv(\n",
      "      (conv): Conv2d(64, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (2): BasicConv(\n",
      "      (conv): Conv2d(96, 96, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(96, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (module_5x5): Sequential(\n",
      "    (0): BasicConv(\n",
      "      (conv): Conv2d(3, 48, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(48, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "    (1): BasicConv(\n",
      "      (conv): Conv2d(48, 64, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      "  (module_pool): Sequential(\n",
      "    (0): MaxPool2d(kernel_size=3, stride=3, padding=1, dilation=1, ceil_mode=False)\n",
      "    (1): BasicConv(\n",
      "      (conv): Conv2d(3, 64, kernel_size=(1, 1), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "      (bn): BatchNorm2d(64, eps=0.001, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.0001\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 0.0005\n",
      ")\n",
      "CrossEntropyLoss()\n"
     ]
    }
   ],
   "source": [
    "# 选择CNN模型\n",
    "cnn = Inception(3)\n",
    "if torch.cuda.is_available():\n",
    "    cnn = cnn.cuda()\n",
    "    print('[Inception]: Use GPU')\n",
    "else:\n",
    "    print('[Inception]: Use CPU')\n",
    "print(cnn)\n",
    "\n",
    "# 优化器参数\n",
    "optimizer = optim.SGD(cnn.parameters(), lr=0.0001, momentum=0.9, weight_decay=0.0005)\n",
    "# optimizer = optim.Adam(cnn.parameters(), lr=0.001, betas=(0.9, 0.99))\n",
    "print(optimizer)\n",
    "# 损失函数: 交叉熵损失函数(CE)\n",
    "criterion = nn.CrossEntropyLoss(size_average=False)\n",
    "print(criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ResNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResBlock(nn.Module):\n",
    "    def __init__(self, inplanes, outplanes, stride=1, downsample=None):\n",
    "        super(ResBlock, self).__init__()\n",
    "        self.conv1 = conv3x3(inplanes, outplanes, stride)\n",
    "        self.bn1 = nn.BatchNorm2d(outplanes)\n",
    "        self.relu = nn.ReLU(True)\n",
    "        self.conv2 = conv3x3(outplanes, planes)\n",
    "        self.bn2 = nn.BatchNorm2d(outplanes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "    \n",
    "    def forward(self, x):\n",
    "        residual = x\n",
    "        x = self.relu(self.bn1(self.conv1(x)))\n",
    "        x = self.relu(self.bn2(self.conv2(x)))\n",
    "        if self.downsample is not None:\n",
    "            residual = self.downsample(x)\n",
    "        x += residual\n",
    "        x = self.relu(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResidualBlock(nn.Module):\n",
    "    '''\n",
    "    submodule: Residual Block\n",
    "    '''\n",
    "    def __init__(self, inchannel, outchannel, stride=1, shortcut=None):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.left = nn.Sequential(\n",
    "            nn.Conv2d(inchannel, outchannel, 3, stride, 1, bias=False),\n",
    "            nn.BatchNorm2d(outchannel),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(outchannel, outchannel, 3, 1, 1, bias=False),\n",
    "            nn.BatchNorm2d(outchannel)\n",
    "        )\n",
    "        self.right = shortcut\n",
    "        \n",
    "    def forward(self, x):\n",
    "        out = self.left(x)\n",
    "        residual = x if self.right is None else self.right(x)\n",
    "        out += residual\n",
    "        return F.relu(out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet(nn.Module):\n",
    "    '''\n",
    "    main module: ResNet34\n",
    "    ResNet34 contains multiple layers,\n",
    "    Each layer contains multiple residual block.\n",
    "    module implements residual block\n",
    "    _make_layer implements layer\n",
    "    '''\n",
    "    def __init__(self, num_classes=1000):\n",
    "        super(ResNet, self).__init__()\n",
    "        # image tranform\n",
    "        self.pre = nn.Sequential(\n",
    "            nn.Conv2d(3, 64, 7, 2, 3, bias=False),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(3, 2, 1)\n",
    "        )\n",
    "        # multiple layers  3, 4, 6, 4 residual blocks\n",
    "        self.layer1 = self._make_layer(64, 128, 3)\n",
    "        self.layer2 = self._make_layer(128, 256, 4, stride=2)\n",
    "        self.layer3 = self._make_layer(256, 512, 6, stride=2)\n",
    "        self.layer4 = self._make_layer(512, 512, 3, stride=2)\n",
    "        \n",
    "        # Classify: Full connection\n",
    "        self.fc = nn.Linear(512, num_classes)\n",
    "        \n",
    "    def _make_layer(self, inchannel, outchannel, block_num, stride=1):\n",
    "        '''\n",
    "        build layer contains multiple residual block\n",
    "        '''\n",
    "        shortcut = nn.Sequential(\n",
    "            nn.Conv2d(inchannel, outchannel, 1, stride, bias=False),\n",
    "            nn.BatchNorm2d(outchannel)\n",
    "        )\n",
    "        layers = []\n",
    "        layers.append(ResidualBlock(inchannel, outchannel, stride, shortcut))\n",
    "        \n",
    "        for i in range(1, block_num):\n",
    "            layers.append(ResidualBlock(outchannel, outchannel))\n",
    "            \n",
    "        return nn.Sequential(*layers)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.pre(x)\n",
    "        \n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "        \n",
    "        x = F.avg_pool2d(x, 7)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        \n",
    "        return self.fc(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            module name  input shape output shape      params memory(MB)              MAdd             Flops  MemRead(B)  MemWrite(B) duration[%]    MemR+W(B)\n",
      "0                 pre.0    3 224 224   64 112 112      9408.0       3.06     235,225,088.0     118,013,952.0    639744.0    3211264.0       1.03%    3851008.0\n",
      "1                 pre.1   64 112 112   64 112 112       128.0       3.06       3,211,264.0       1,605,632.0   3211776.0    3211264.0       0.42%    6423040.0\n",
      "2                 pre.2   64 112 112   64 112 112         0.0       3.06         802,816.0         802,816.0   3211264.0    3211264.0       0.09%    6422528.0\n",
      "3                 pre.3   64 112 112   64  56  56         0.0       0.77       1,605,632.0         802,816.0   3211264.0     802816.0       3.26%    4014080.0\n",
      "4       layer1.0.left.0   64  56  56  128  56  56     73728.0       1.53     462,020,608.0     231,211,008.0   1097728.0    1605632.0       0.99%    2703360.0\n",
      "5       layer1.0.left.1  128  56  56  128  56  56       256.0       1.53       1,605,632.0         802,816.0   1606656.0    1605632.0       0.26%    3212288.0\n",
      "6       layer1.0.left.2  128  56  56  128  56  56         0.0       1.53         401,408.0         401,408.0   1605632.0    1605632.0       0.03%    3211264.0\n",
      "7       layer1.0.left.3  128  56  56  128  56  56    147456.0       1.53     924,442,624.0     462,422,016.0   2195456.0    1605632.0       2.10%    3801088.0\n",
      "8       layer1.0.left.4  128  56  56  128  56  56       256.0       1.53       1,605,632.0         802,816.0   1606656.0    1605632.0       0.22%    3212288.0\n",
      "9      layer1.0.right.0   64  56  56  128  56  56      8192.0       1.53      50,978,816.0      25,690,112.0    835584.0    1605632.0       0.50%    2441216.0\n",
      "10     layer1.0.right.1  128  56  56  128  56  56       256.0       1.53       1,605,632.0         802,816.0   1606656.0    1605632.0       0.25%    3212288.0\n",
      "11      layer1.1.left.0  128  56  56  128  56  56    147456.0       1.53     924,442,624.0     462,422,016.0   2195456.0    1605632.0       1.58%    3801088.0\n",
      "12      layer1.1.left.1  128  56  56  128  56  56       256.0       1.53       1,605,632.0         802,816.0   1606656.0    1605632.0       0.35%    3212288.0\n",
      "13      layer1.1.left.2  128  56  56  128  56  56         0.0       1.53         401,408.0         401,408.0   1605632.0    1605632.0       0.05%    3211264.0\n",
      "14      layer1.1.left.3  128  56  56  128  56  56    147456.0       1.53     924,442,624.0     462,422,016.0   2195456.0    1605632.0       2.03%    3801088.0\n",
      "15      layer1.1.left.4  128  56  56  128  56  56       256.0       1.53       1,605,632.0         802,816.0   1606656.0    1605632.0       0.30%    3212288.0\n",
      "16      layer1.2.left.0  128  56  56  128  56  56    147456.0       1.53     924,442,624.0     462,422,016.0   2195456.0    1605632.0       1.63%    3801088.0\n",
      "17      layer1.2.left.1  128  56  56  128  56  56       256.0       1.53       1,605,632.0         802,816.0   1606656.0    1605632.0       0.30%    3212288.0\n",
      "18      layer1.2.left.2  128  56  56  128  56  56         0.0       1.53         401,408.0         401,408.0   1605632.0    1605632.0       0.03%    3211264.0\n",
      "19      layer1.2.left.3  128  56  56  128  56  56    147456.0       1.53     924,442,624.0     462,422,016.0   2195456.0    1605632.0       1.53%    3801088.0\n",
      "20      layer1.2.left.4  128  56  56  128  56  56       256.0       1.53       1,605,632.0         802,816.0   1606656.0    1605632.0       0.27%    3212288.0\n",
      "21      layer2.0.left.0  128  56  56  256  28  28    294912.0       0.77     462,221,312.0     231,211,008.0   2785280.0     802816.0       1.03%    3588096.0\n",
      "22      layer2.0.left.1  256  28  28  256  28  28       512.0       0.77         802,816.0         401,408.0    804864.0     802816.0       0.15%    1607680.0\n",
      "23      layer2.0.left.2  256  28  28  256  28  28         0.0       0.77         200,704.0         200,704.0    802816.0     802816.0       0.02%    1605632.0\n",
      "24      layer2.0.left.3  256  28  28  256  28  28    589824.0       0.77     924,643,328.0     462,422,016.0   3162112.0     802816.0       2.09%    3964928.0\n",
      "25      layer2.0.left.4  256  28  28  256  28  28       512.0       0.77         802,816.0         401,408.0    804864.0     802816.0       0.30%    1607680.0\n",
      "26     layer2.0.right.0  128  56  56  256  28  28     32768.0       0.77      51,179,520.0      25,690,112.0   1736704.0     802816.0       0.65%    2539520.0\n",
      "27     layer2.0.right.1  256  28  28  256  28  28       512.0       0.77         802,816.0         401,408.0    804864.0     802816.0       0.16%    1607680.0\n",
      "28      layer2.1.left.0  256  28  28  256  28  28    589824.0       0.77     924,643,328.0     462,422,016.0   3162112.0     802816.0       1.80%    3964928.0\n",
      "29      layer2.1.left.1  256  28  28  256  28  28       512.0       0.77         802,816.0         401,408.0    804864.0     802816.0       0.19%    1607680.0\n",
      "30      layer2.1.left.2  256  28  28  256  28  28         0.0       0.77         200,704.0         200,704.0    802816.0     802816.0       0.02%    1605632.0\n",
      "31      layer2.1.left.3  256  28  28  256  28  28    589824.0       0.77     924,643,328.0     462,422,016.0   3162112.0     802816.0       1.43%    3964928.0\n",
      "32      layer2.1.left.4  256  28  28  256  28  28       512.0       0.77         802,816.0         401,408.0    804864.0     802816.0       0.19%    1607680.0\n",
      "33      layer2.2.left.0  256  28  28  256  28  28    589824.0       0.77     924,643,328.0     462,422,016.0   3162112.0     802816.0       2.14%    3964928.0\n",
      "34      layer2.2.left.1  256  28  28  256  28  28       512.0       0.77         802,816.0         401,408.0    804864.0     802816.0       0.15%    1607680.0\n",
      "35      layer2.2.left.2  256  28  28  256  28  28         0.0       0.77         200,704.0         200,704.0    802816.0     802816.0       0.02%    1605632.0\n",
      "36      layer2.2.left.3  256  28  28  256  28  28    589824.0       0.77     924,643,328.0     462,422,016.0   3162112.0     802816.0       1.53%    3964928.0\n",
      "37      layer2.2.left.4  256  28  28  256  28  28       512.0       0.77         802,816.0         401,408.0    804864.0     802816.0       0.21%    1607680.0\n",
      "38      layer2.3.left.0  256  28  28  256  28  28    589824.0       0.77     924,643,328.0     462,422,016.0   3162112.0     802816.0       1.82%    3964928.0\n",
      "39      layer2.3.left.1  256  28  28  256  28  28       512.0       0.77         802,816.0         401,408.0    804864.0     802816.0       0.21%    1607680.0\n",
      "40      layer2.3.left.2  256  28  28  256  28  28         0.0       0.77         200,704.0         200,704.0    802816.0     802816.0       0.14%    1605632.0\n",
      "41      layer2.3.left.3  256  28  28  256  28  28    589824.0       0.77     924,643,328.0     462,422,016.0   3162112.0     802816.0       1.54%    3964928.0\n",
      "42      layer2.3.left.4  256  28  28  256  28  28       512.0       0.77         802,816.0         401,408.0    804864.0     802816.0       0.21%    1607680.0\n",
      "43      layer3.0.left.0  256  28  28  512  14  14   1179648.0       0.38     462,321,664.0     231,211,008.0   5521408.0     401408.0       1.27%    5922816.0\n",
      "44      layer3.0.left.1  512  14  14  512  14  14      1024.0       0.38         401,408.0         200,704.0    405504.0     401408.0       0.17%     806912.0\n",
      "45      layer3.0.left.2  512  14  14  512  14  14         0.0       0.38         100,352.0         100,352.0    401408.0     401408.0       0.02%     802816.0\n",
      "46      layer3.0.left.3  512  14  14  512  14  14   2359296.0       0.38     924,743,680.0     462,422,016.0   9838592.0     401408.0       2.00%   10240000.0\n",
      "47      layer3.0.left.4  512  14  14  512  14  14      1024.0       0.38         401,408.0         200,704.0    405504.0     401408.0       0.17%     806912.0\n",
      "48     layer3.0.right.0  256  28  28  512  14  14    131072.0       0.38      51,279,872.0      25,690,112.0   1327104.0     401408.0       0.55%    1728512.0\n",
      "49     layer3.0.right.1  512  14  14  512  14  14      1024.0       0.38         401,408.0         200,704.0    405504.0     401408.0       0.13%     806912.0\n",
      "50      layer3.1.left.0  512  14  14  512  14  14   2359296.0       0.38     924,743,680.0     462,422,016.0   9838592.0     401408.0       2.84%   10240000.0\n",
      "51      layer3.1.left.1  512  14  14  512  14  14      1024.0       0.38         401,408.0         200,704.0    405504.0     401408.0       0.14%     806912.0\n",
      "52      layer3.1.left.2  512  14  14  512  14  14         0.0       0.38         100,352.0         100,352.0    401408.0     401408.0       0.03%     802816.0\n",
      "53      layer3.1.left.3  512  14  14  512  14  14   2359296.0       0.38     924,743,680.0     462,422,016.0   9838592.0     401408.0       2.67%   10240000.0\n",
      "54      layer3.1.left.4  512  14  14  512  14  14      1024.0       0.38         401,408.0         200,704.0    405504.0     401408.0       0.14%     806912.0\n",
      "55      layer3.2.left.0  512  14  14  512  14  14   2359296.0       0.38     924,743,680.0     462,422,016.0   9838592.0     401408.0       2.32%   10240000.0\n",
      "56      layer3.2.left.1  512  14  14  512  14  14      1024.0       0.38         401,408.0         200,704.0    405504.0     401408.0       0.15%     806912.0\n",
      "57      layer3.2.left.2  512  14  14  512  14  14         0.0       0.38         100,352.0         100,352.0    401408.0     401408.0       0.02%     802816.0\n",
      "58      layer3.2.left.3  512  14  14  512  14  14   2359296.0       0.38     924,743,680.0     462,422,016.0   9838592.0     401408.0       2.13%   10240000.0\n",
      "59      layer3.2.left.4  512  14  14  512  14  14      1024.0       0.38         401,408.0         200,704.0    405504.0     401408.0       0.19%     806912.0\n",
      "60      layer3.3.left.0  512  14  14  512  14  14   2359296.0       0.38     924,743,680.0     462,422,016.0   9838592.0     401408.0       2.31%   10240000.0\n",
      "61      layer3.3.left.1  512  14  14  512  14  14      1024.0       0.38         401,408.0         200,704.0    405504.0     401408.0       0.17%     806912.0\n",
      "62      layer3.3.left.2  512  14  14  512  14  14         0.0       0.38         100,352.0         100,352.0    401408.0     401408.0       0.02%     802816.0\n",
      "63      layer3.3.left.3  512  14  14  512  14  14   2359296.0       0.38     924,743,680.0     462,422,016.0   9838592.0     401408.0       2.28%   10240000.0\n",
      "64      layer3.3.left.4  512  14  14  512  14  14      1024.0       0.38         401,408.0         200,704.0    405504.0     401408.0       0.12%     806912.0\n",
      "65      layer3.4.left.0  512  14  14  512  14  14   2359296.0       0.38     924,743,680.0     462,422,016.0   9838592.0     401408.0       2.51%   10240000.0\n",
      "66      layer3.4.left.1  512  14  14  512  14  14      1024.0       0.38         401,408.0         200,704.0    405504.0     401408.0       0.12%     806912.0\n",
      "67      layer3.4.left.2  512  14  14  512  14  14         0.0       0.38         100,352.0         100,352.0    401408.0     401408.0       0.02%     802816.0\n",
      "68      layer3.4.left.3  512  14  14  512  14  14   2359296.0       0.38     924,743,680.0     462,422,016.0   9838592.0     401408.0       1.96%   10240000.0\n",
      "69      layer3.4.left.4  512  14  14  512  14  14      1024.0       0.38         401,408.0         200,704.0    405504.0     401408.0       0.17%     806912.0\n",
      "70      layer3.5.left.0  512  14  14  512  14  14   2359296.0       0.38     924,743,680.0     462,422,016.0   9838592.0     401408.0       2.29%   10240000.0\n",
      "71      layer3.5.left.1  512  14  14  512  14  14      1024.0       0.38         401,408.0         200,704.0    405504.0     401408.0       0.17%     806912.0\n",
      "72      layer3.5.left.2  512  14  14  512  14  14         0.0       0.38         100,352.0         100,352.0    401408.0     401408.0       0.02%     802816.0\n",
      "73      layer3.5.left.3  512  14  14  512  14  14   2359296.0       0.38     924,743,680.0     462,422,016.0   9838592.0     401408.0       2.53%   10240000.0\n",
      "74      layer3.5.left.4  512  14  14  512  14  14      1024.0       0.38         401,408.0         200,704.0    405504.0     401408.0       0.14%     806912.0\n",
      "75      layer4.0.left.0  512  14  14  512   7   7   2359296.0       0.10     231,185,920.0     115,605,504.0   9838592.0     100352.0       1.32%    9938944.0\n",
      "76      layer4.0.left.1  512   7   7  512   7   7      1024.0       0.10         100,352.0          50,176.0    104448.0     100352.0       0.13%     204800.0\n",
      "77      layer4.0.left.2  512   7   7  512   7   7         0.0       0.10          25,088.0          25,088.0    100352.0     100352.0       0.01%     200704.0\n",
      "78      layer4.0.left.3  512   7   7  512   7   7   2359296.0       0.10     231,185,920.0     115,605,504.0   9537536.0     100352.0       1.34%    9637888.0\n",
      "79      layer4.0.left.4  512   7   7  512   7   7      1024.0       0.10         100,352.0          50,176.0    104448.0     100352.0       0.14%     204800.0\n",
      "80     layer4.0.right.0  512  14  14  512   7   7    262144.0       0.10      25,665,024.0      12,845,056.0   1449984.0     100352.0       0.40%    1550336.0\n",
      "81     layer4.0.right.1  512   7   7  512   7   7      1024.0       0.10         100,352.0          50,176.0    104448.0     100352.0       0.10%     204800.0\n",
      "82      layer4.1.left.0  512   7   7  512   7   7   2359296.0       0.10     231,185,920.0     115,605,504.0   9537536.0     100352.0       1.39%    9637888.0\n",
      "83      layer4.1.left.1  512   7   7  512   7   7      1024.0       0.10         100,352.0          50,176.0    104448.0     100352.0       0.12%     204800.0\n",
      "84      layer4.1.left.2  512   7   7  512   7   7         0.0       0.10          25,088.0          25,088.0    100352.0     100352.0       0.02%     200704.0\n",
      "85      layer4.1.left.3  512   7   7  512   7   7   2359296.0       0.10     231,185,920.0     115,605,504.0   9537536.0     100352.0       1.19%    9637888.0\n",
      "86      layer4.1.left.4  512   7   7  512   7   7      1024.0       0.10         100,352.0          50,176.0    104448.0     100352.0       0.11%     204800.0\n",
      "87      layer4.2.left.0  512   7   7  512   7   7   2359296.0       0.10     231,185,920.0     115,605,504.0   9537536.0     100352.0       1.22%    9637888.0\n",
      "88      layer4.2.left.1  512   7   7  512   7   7      1024.0       0.10         100,352.0          50,176.0    104448.0     100352.0       0.13%     204800.0\n",
      "89      layer4.2.left.2  512   7   7  512   7   7         0.0       0.10          25,088.0          25,088.0    100352.0     100352.0       0.02%     200704.0\n",
      "90      layer4.2.left.3  512   7   7  512   7   7   2359296.0       0.10     231,185,920.0     115,605,504.0   9537536.0     100352.0       1.30%    9637888.0\n",
      "91      layer4.2.left.4  512   7   7  512   7   7      1024.0       0.10         100,352.0          50,176.0    104448.0     100352.0       0.13%     204800.0\n",
      "92                   fc          512           10      5130.0       0.00          10,230.0           5,120.0     22568.0         40.0      27.96%      22608.0\n",
      "total                                              46998090.0      66.71  24,487,604,214.0  12,248,995,328.0     22568.0         40.0     100.00%  329089360.0\n",
      "==============================================================================================================================================================\n",
      "Total params: 46,998,090\n",
      "--------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "Total memory: 66.71MB\n",
      "Total MAdd: 24.49GMAdd\n",
      "Total Flops: 12.25GFlops\n",
      "Total MemR+W: 313.84MB\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cnn = ResNet(10)\n",
    "stat(cnn, (3, 224, 224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ResNet]: Use GPU\n",
      "ResNet(\n",
      "  (pre): Sequential(\n",
      "    (0): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
      "    (1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "    (2): ReLU(inplace)\n",
      "    (3): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
      "  )\n",
      "  (layer1): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (left): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace)\n",
      "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (right): Sequential(\n",
      "        (0): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (left): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace)\n",
      "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): ResidualBlock(\n",
      "      (left): Sequential(\n",
      "        (0): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace)\n",
      "        (3): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer2): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (left): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace)\n",
      "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (right): Sequential(\n",
      "        (0): Conv2d(128, 256, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (left): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace)\n",
      "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): ResidualBlock(\n",
      "      (left): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace)\n",
      "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): ResidualBlock(\n",
      "      (left): Sequential(\n",
      "        (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace)\n",
      "        (3): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer3): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (left): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace)\n",
      "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (right): Sequential(\n",
      "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (left): Sequential(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace)\n",
      "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): ResidualBlock(\n",
      "      (left): Sequential(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace)\n",
      "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (3): ResidualBlock(\n",
      "      (left): Sequential(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace)\n",
      "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (4): ResidualBlock(\n",
      "      (left): Sequential(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace)\n",
      "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (5): ResidualBlock(\n",
      "      (left): Sequential(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace)\n",
      "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (layer4): Sequential(\n",
      "    (0): ResidualBlock(\n",
      "      (left): Sequential(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace)\n",
      "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "      (right): Sequential(\n",
      "        (0): Conv2d(512, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (1): ResidualBlock(\n",
      "      (left): Sequential(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace)\n",
      "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "    (2): ResidualBlock(\n",
      "      (left): Sequential(\n",
      "        (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "        (2): ReLU(inplace)\n",
      "        (3): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
      "        (4): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=512, out_features=10, bias=True)\n",
      ")\n",
      "SGD (\n",
      "Parameter Group 0\n",
      "    dampening: 0\n",
      "    lr: 0.0001\n",
      "    momentum: 0.9\n",
      "    nesterov: False\n",
      "    weight_decay: 0.0005\n",
      ")\n",
      "CrossEntropyLoss()\n"
     ]
    }
   ],
   "source": [
    "# 选择CNN模型\n",
    "cnn = ResNet(10)\n",
    "if torch.cuda.is_available():\n",
    "    cnn = cnn.cuda()\n",
    "    print('[ResNet]: Use GPU')\n",
    "else:\n",
    "    print('[ResNet]: Use CPU')\n",
    "print(cnn)\n",
    "\n",
    "# 优化器参数\n",
    "optimizer = optim.SGD(cnn.parameters(), lr=0.0001, momentum=0.9, weight_decay=0.0005)\n",
    "# optimizer = optim.Adam(cnn.parameters(), lr=0.001, betas=(0.9, 0.99))\n",
    "print(optimizer)\n",
    "# 损失函数: 交叉熵损失函数(CE)\n",
    "criterion = nn.CrossEntropyLoss(size_average=False)\n",
    "print(criterion)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Result\n",
    "\n",
    "CNN|Time|Accuracy\n",
    ":-:|:-:|:-:\n",
    "LeNet-5|40.8s|99.21%\n",
    "AlexNet|96.2s|99.31%\n",
    "VGG-16| --- | ---\n",
    "VGG-19| --- | ---\n",
    "GoogLeNet| --- | ---\n",
    "ResNet| --- | ---"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
